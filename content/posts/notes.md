+++
title = 'notes'
date = 2024-05-10
draft = false
+++

> this contains my notes, me logging my thoughts on projects/goals/life etc as i go through them along the way. *it would be edited constantly & the latest log would always be on top.*

- 10may24

been a lot of gap but i'll try to consistently write here. i graduated, was shifting places, travelling, not a lot of work done but i'm back. my macbook's charging adapter is acting funny, so apple care folks told me to submit for 4-5 days & may potentially also format my data. like wth? lotta time spent in backing up data in usb.

now, work:
i have a list of interesting work to explore. i'm slow, like a damn tortoise. i need speed, become speed because "slowness once justifies slowness elsewhere". what i'm absolutely sure of that becoming genuinely cracked is the true path i need to take. there're a lot of talkers on X (sometimes i feel i'm them too) but raw quality of a diamond is what matters. so son, "if you gotta shine like a son, first burn like a sun". its fun, not that bad though. I CAN JUST DO THINGS & BECOME GOOD.

the work to explore includes different "arcs":
![todos ss](/assets/notes/10may24.1.webp)

i'm excited by the fact that i can just train models on my mac with mlx, that unlocks so much. gotta try train smol million param models beyond chinchilla (since llama3), then vision & so on. also, "tokenizer is a necessary evil". karpathy also *delved* into it, so gotta do that too.

everything aside, back to home & gotta get back to my boxing training.

- 19apr24

late update but lagllama + encoder-decoder model is done.

- 03apr24

have to complete the time-series-transformers project (also, college's major project) which includes multivariate data: streamflow, rainfall, temperature etc. i've been procrastinating it on forever & my friend did his part of LSTM, i gotta ship something by end of today. i've a hf's decoder probabilistic one but damn its dataset cleaning has been tough, also i've an encoder-decoder in mind, spatio-temporal transformer's "Long transformer paper" i read much much earlier and yeah, a latest foundation lagllama model (gotta finetune).

by today gotta do the: probablistic decoder + encoder-decoder.

life is all a game of endurance, always has been. i'm thinking of getting this published in a Q1, now i've the support of a great prof, let's not waste this.